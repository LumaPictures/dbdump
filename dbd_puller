#!/usr/bin/env python
import base64
import csv
import datetime
import httplib
import json
import logging
import sys
try:
    import cStringIO as StringIO
except ImportError:
    import StringIO
import time
import urllib

import argparse
try:
    import argcomplete
except ImportError:
    argcomplete = None
from dateutil import tz
import MySQLdb
import _mysql_exceptions

__version__ = '1.0.0'

# Default args
DBDUMP_DEFAULT_PORT = 8888
DBDUMP_DEFAULT_TIMEOUT = 3 * 60
DB_DEFAULT_PORT = 3306
NULL_MAGIC_STRING = '$$NULL$$'

logger = logging.getLogger(__name__)
quote = MySQLdb.escape_string
arguments = {}


def unzip(d):
    """
    Returns (list_of_keys, list_of_values) in the same order they were in the dictionary.
    Opposite of zip()
    """
    keys = []
    values = []

    for k, v in d.iteritems():
        keys.append(k)
        values.append(v)

    return keys, values


def transform_column_data(column_data, quote_char="'"):
    if column_data is None or column_data == NULL_MAGIC_STRING:
        return 'null'
    else:
        return quote_char + quote(str(column_data)) + quote_char


def sync_table(dbdump_conn, db_conn, database, table, ts_column=None):
    logger.info('Synchronizing table %s...' % table)

    query = {}
    if ts_column is None:
        ts_column = get_autoupdate_timestamp_columns(db_conn, database).get(table)
        if ts_column is None:
            logger.error('Table "%s" does not exist in database' % table)
            return

    # Get latest value
    cursor = db_conn.cursor()
    cursor.execute("select max(`%s`) from `%s`" % (quote(ts_column), quote(table)))
    sync_meta = cursor.fetchone()
    if sync_meta is not None:
        last_sync_at = sync_meta[0]
        if last_sync_at is not None:
            query['since'] = last_sync_at \
                .replace(tzinfo=get_db_timezone(db_conn)) \
                .astimezone(tz.gettz('GMT')) \
                .strftime('%Y-%m-%d %H:%M:%S')

    primary_key = get_primary_key_columns(db_conn, database, table)
    #logger.info('Primary key: %s' % ', '.join(primary_key))

    logger.info('Querying %s since %s' % (table, query.get('since')))
    dbdump_conn.request('GET', '/%s/%s/?%s' % (database, table, urllib.urlencode(query.items())), 
                        headers=build_headers())
    resp = dbdump_conn.getresponse()
    body = resp.read()

    if not (199 < resp.status < 300):  # Error (non-2xx) response?
        raise ValueError('Error %d fetching table rows: %s' % (resp.status, body))

    if not resp.getheader('Content-Type') == 'text/csv':
        raise ValueError('Error: Response not in CSV format')

    string_reader = StringIO.StringIO(body)
    csv_reader = csv.reader(string_reader, 
                            delimiter=',', 
                            quotechar='"', 
                            doublequote=False,
                            quoting=csv.QUOTE_ALL,
                            escapechar="\\", 
                            lineterminator='\n',
                            strict=True)

    excluded_columns = get_excluded_columns().get(table) or set()
    logger.debug('Excluded columns for %s: %s' % (table, ', '.join(excluded_columns)))

    try:
        column_names = csv_reader.next()
        inserts = 0
        updates = 0
        for _row in csv_reader:

            # Blank row?
            if len(_row) == 0:
                continue

            row = dict(zip(column_names, _row))
            key_string = ','.join([row[k] for k in primary_key])

            # Determine if an local record exists
            local_record = get_local_record(db_conn, table, row, primary_key)
            if local_record is None:
                # Insert
                logger.info('Adding %s %s (updated %s)' % (table, key_string, row[ts_column]))
                query = "insert into `%s` (`%s`) values (%s)" % (
                    quote(table), 
                    '`, `'.join([quote(k) for k in column_names]), 
                    ", ".join([transform_column_data(v) 
                               for v in _row]))
                inserts += 1
            else:
                # Update
                logger.info('Updating %s %s (updated %s)' % (table, key_string, row[ts_column]))
                query = "update `%s` set %s where %s" % (
                    quote(table), 
                    ', '.join(["`%s` = %s" % (quote(k), transform_column_data(v)) 
                               for k, v in row.items() 
                               if k not in primary_key]), 
                    ' and '.join(["`%s` = %s" % (quote(k), transform_column_data(row[k])) 
                                  for k in primary_key]))
                updates += 1

            logger.debug('Query: %s' % query)
            try:
                cursor.execute(query)
            except _mysql_exceptions.MySQLError, ex:
                logger.error('Error updating database: %s' % ex)
                if not arguments['ignore_database_errors']:
                    raise ex

        db_conn.commit()

        logger.info('Finished synchronization (%d inserts, %d updates)' % (inserts, updates))

    except StopIteration:
        logger.info('Already up-to-date')

    finally:
        string_reader.close()
        del string_reader

    # TODO: sync data


def get_excluded_columns():
    """
    Returns column exclusions as { table: set(column_names) }
    """
    excluded_column_expr = arguments['exclude_columns']
    if excluded_column_expr is None:
        return {}

    exclude_columns = {}
    for table_column in excluded_column_expr.split(','):
        table_name, column_name = table_column.split('.', 2)
        if table_name not in exclude_columns:
            exclude_columns[table_name] = set()
        exclude_columns[table_name].add(column_name)

    return exclude_columns


def get_local_record(db_conn, table, row, primary_key):
    where_stmt = ' and '.join(["`%s` = '%s'" % (quote(k), quote(str(row[k]))) for k in primary_key])

    cursor = db_conn.cursor()
    cursor.execute("select `%s` from `%s` where %s" % ('`, `'.join(row.keys()), quote(table), where_stmt))
    result = cursor.fetchone()
    if result is not None:
        return zip(row.keys(), result)


def get_primary_key_columns(conn, database, table):
    cursor = conn.cursor()
    cursor.execute("""
        select
            kcu.column_name
        from information_schema.table_constraints tc
        join information_schema.key_column_usage kcu on
            kcu.constraint_schema = tc.constraint_schema and
            kcu.constraint_name = tc.constraint_name and
            kcu.table_schema = tc.table_schema and
            kcu.table_name = tc.table_name
        where
            tc.constraint_type = 'PRIMARY KEY' and
            tc.table_schema = %s and
            tc.table_name = %s
        order by
            kcu.ordinal_position
        """, (database, table))

    return [row[0] for row in cursor.fetchall()]


def get_autoupdate_timestamp_columns(conn, database):
    """
    Returns a dict of {table_name: timestamped_column_name} for only the
    tables that have auto-updated timestamps.
    """
    cursor = conn.cursor()
    cursor.execute("""
        select
            table_name,
            column_name
        from information_schema.columns
        where
            table_schema = %s and
            data_type = 'timestamp' and
            extra = 'on update CURRENT_TIMESTAMP'
        order by table_name
        """, (database,))

    # There shouldn't be any duplicate entries for tables in this list, since
    # MySQL specifies that there can only be one auto-updated timestamp column.
    return dict([(row[0], row[1]) for row in cursor.fetchall()])


def build_headers():
    headers = {'Accept': 'text/csv'}

    if 'source_username' in arguments and 'source_password' in arguments:
        # Authenticate request
        headers['Authorization'] = 'Basic %s' % \
            base64.b64encode('%(source_username)s:%(source_password)s' % arguments)

    return headers


def get_tables_to_sync(conn, database):
    tables = arguments['tables']
    if tables:
        # Specific list of tables
        return tables.split(',')
    else:
        # Get all tables that dbd_server supports
        conn.request('GET', '/%s/' % database, headers=build_headers())
        resp = conn.getresponse()
        body = resp.read()

        if not (199 < resp.status < 300):  # Error (non-2xx) response?
            raise ValueError('Error %d fetching tables: %s' % (resp.status, body))

        if not resp.getheader('Content-Type') == 'text/csv':
            raise ValueError('Error: Response not in CSV format')

        return map(json.loads, body.splitlines()[1:])


def get_dbdump_connection():
    return httplib.HTTPConnection('%(source_host)s:%(source_port)d' % arguments, 
                                  timeout=arguments['source_timeout'])


def get_system_timezone():
    tz_name = time.strftime('%Z', time.gmtime())
    return tz.gettz(tz_name)


def get_db_timezone(conn):
    cursor = conn.cursor()
    cursor.execute("select @@session.time_zone")

    row = cursor.fetchone()
    if row is None or row[0] == 'SYSTEM':
        return get_system_timezone()

    return tz.gettz(row[0])


def get_db_connection():
    """
    Gets a connection to the database server.
    """
    conn = MySQLdb.connect(host=arguments['dest_host'], 
                           port=arguments['dest_port'],
                           user=arguments['dest_username'], 
                           passwd=arguments['dest_password'],
                           db=arguments['dest_database'])
    #conn.autocommit(False)
    return conn


def init_logger():
    logging.basicConfig(level=logging.DEBUG if arguments['debug'] else logging.INFO,
                    format='%(asctime)s | %(levelname)-5s | %(name)s: %(message)s')


def main():
    init_logger()
    debug = arguments['debug']

    #logger.debug('Arguments: %s' % json.dumps(arguments))

    poll_interval = arguments['poll_interval']
    while True:
        dbdump_conn = get_dbdump_connection()
        db_conn = get_db_connection()
        try:
            database = arguments['source_database']
            tables = get_tables_to_sync(dbdump_conn, database)

            logger.debug('Supported tables: %s' % ', '.join(tables))
            for table_spec in tables:
                if '.' in table_spec:
                    table_name, ts_column = table_spec.split('.', 2)
                else:
                    table_name = table
                    ts_column = None
                sync_table(dbdump_conn, db_conn, database, table_name, ts_column)

        finally:
            db_conn.close()
            dbdump_conn.close()

        if poll_interval is None:
            break

        logger.info('Polling again in %d seconds...' % poll_interval)
        time.sleep(poll_interval)

    return 0


if __name__ == '__main__':
    parser = argparse.ArgumentParser(#prog='syncwebdb',
                                     description='Pulls changes from a dbd_pusher server')

    # Server options
    server_group = parser.add_argument_group('Source', 'Source dbd_server (that changes are read from)')
    server_group.add_argument('--source-host', metavar='HOST_OR_IP', required=True,
                              help='dbdump web server')
    server_group.add_argument('--source-port', metavar='PORT', type=int, default=DBDUMP_DEFAULT_PORT, 
                              help='dbdump web server port (default: %d)' % DBDUMP_DEFAULT_PORT)
    server_group.add_argument('--source-username', metavar='USERNAME',
                              help='dbdump username')
    server_group.add_argument('--source-password', metavar='PASSWORD',
                              help='dbdump password')
    server_group.add_argument('--source-database', metavar='DATABASE', required=True,
                              help='Source database name')
    server_group.add_argument('--source-timeout', metavar='SECONDS', type=int, default=DBDUMP_DEFAULT_TIMEOUT,
                              help='Seconds to wait before timing out connection attempt (default: %d)' % DBDUMP_DEFAULT_TIMEOUT)

    # Database client
    db_group = parser.add_argument_group('Destination', 'Destination MySQL database server (that changes get written to)')
    db_group.add_argument('--dest-host', metavar='HOST_OR_IP', required=True, 
                          help='Database server host')
    db_group.add_argument('--dest-port', metavar='PORT', type=int, default=DB_DEFAULT_PORT,
                          help='Database server port (default: %d)' % DB_DEFAULT_PORT)
    db_group.add_argument('--dest-username', metavar='USERNAME', required=True,
                          help='Database user')
    db_group.add_argument('--dest-password', metavar='PASSWORD', default=None,
                          help='Database password')
    db_group.add_argument('--dest-database', metavar='DATABASE', required=True,
                          help='Destination database name')
    db_group.add_argument('--ignore-database-errors', action='store_true',
                          help='Log and ignore MySQL database errors that occur when manipulating data')

    # Sync options
    sync_group = parser.add_argument_group('Sync', 'Synchronization options')
    sync_group.add_argument('-t', '--tables', metavar='table1,tableN,...',
                            help='Ordered list of tables to sync (default: all tables that dbd_server supports)')
    sync_group.add_argument('-x', '--exclude-columns', metavar='table1.column1,tableN.columnN,...',
                            help='Columns to exclude from synchronization')
    sync_group.add_argument('--poll-interval', metavar='seconds', type=int, default=None,
                            help='Polling interval between pull requests (if unspecified, polling will be disabled)')

    # Misc options
    parser.add_argument('-d', '--debug', action='store_true',
                        help='Enable debug mode')
    parser.add_argument('--version', action='version',
                        version='%%(prog)s (version %s)' % __version__,
                        help='Display version and exit')

    if argcomplete:
        argcomplete.autocomplete(parser)

    arguments = vars(parser.parse_args())
    sys.exit(main())
